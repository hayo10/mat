{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8a750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from _aux_mamba import get_tokenizer, get_model\n",
    "file1 = 'experiment/mamba-130m-hf/xsum_r2_scores.pickle'\n",
    "with open(file1, 'rb') as file:\n",
    "    r2_scores = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba9bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cab64cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaForCausalLM(\n",
      "  (backbone): MambaModel(\n",
      "    (embeddings): Embedding(50280, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x MambaBlock(\n",
      "        (norm): MambaRMSNorm()\n",
      "        (mixer): MambaMixer(\n",
      "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
      "          (act): SiLU()\n",
      "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
      "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
      "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
      "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_f): MambaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "basemodel = get_model('state-spaces/mamba-130m-hf')\n",
    "print(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc631cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('state-spaces/mamba-130m-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"EdinburghNLP/xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2c7574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score_mat': {(0, 1): 0.9137277636816773,\n",
       "  (0, 2): 0.8363533823441148,\n",
       "  (0, 3): 0.7639078281120281,\n",
       "  (0, 4): 0.6969123391105859,\n",
       "  (0, 5): 0.6413095813975179,\n",
       "  (0, 6): 0.5984488412060005,\n",
       "  (0, 7): 0.57220997524843,\n",
       "  (0, 8): 0.5497972424181198,\n",
       "  (0, 9): 0.5269987274725728,\n",
       "  (0, 10): 0.49874316572782457,\n",
       "  (0, 11): 0.4835394259239571,\n",
       "  (0, 12): 0.4752907011968935,\n",
       "  (0, 13): 0.46539522829708374,\n",
       "  (0, 14): 0.4562043324814892,\n",
       "  (0, 15): 0.4438146654157231,\n",
       "  (0, 16): 0.4400127271305503,\n",
       "  (0, 17): 0.4302339148907544,\n",
       "  (0, 18): 0.38291196375159836,\n",
       "  (0, 19): 0.36007627911266,\n",
       "  (0, 20): 0.35636580390707256,\n",
       "  (0, 21): 0.32873609802010173,\n",
       "  (0, 22): 0.3250094567271773,\n",
       "  (0, 23): 0.34676111654246883,\n",
       "  (0, 24): 0.38709683668414857,\n",
       "  (1, 2): 0.9148679551217208,\n",
       "  (1, 3): 0.8348227200566628,\n",
       "  (1, 4): 0.7595382727357537,\n",
       "  (1, 5): 0.6966753390389995,\n",
       "  (1, 6): 0.6473725061337524,\n",
       "  (1, 7): 0.616333580294515,\n",
       "  (1, 8): 0.5911228107650204,\n",
       "  (1, 9): 0.5661953700388924,\n",
       "  (1, 10): 0.5364845523574039,\n",
       "  (1, 11): 0.5191945424891223,\n",
       "  (1, 12): 0.5100479761111107,\n",
       "  (1, 13): 0.49885389753081605,\n",
       "  (1, 14): 0.48810917190620934,\n",
       "  (1, 15): 0.47422486198323294,\n",
       "  (1, 16): 0.46847236709366374,\n",
       "  (1, 17): 0.4577075807198742,\n",
       "  (1, 18): 0.41079078944398617,\n",
       "  (1, 19): 0.38711318558486996,\n",
       "  (1, 20): 0.38359760945420346,\n",
       "  (1, 21): 0.3574130759185335,\n",
       "  (1, 22): 0.35435845778156505,\n",
       "  (1, 23): 0.3776797495875095,\n",
       "  (1, 24): 0.4145049506727256,\n",
       "  (2, 3): 0.9104294079721985,\n",
       "  (2, 4): 0.8248061028862678,\n",
       "  (2, 5): 0.755477353514233,\n",
       "  (2, 6): 0.6967154630660213,\n",
       "  (2, 7): 0.6576483666910159,\n",
       "  (2, 8): 0.6279048554936586,\n",
       "  (2, 9): 0.5992480827875472,\n",
       "  (2, 10): 0.5672506345443882,\n",
       "  (2, 11): 0.5480606066895994,\n",
       "  (2, 12): 0.5385835853264418,\n",
       "  (2, 13): 0.5268455610204117,\n",
       "  (2, 14): 0.5148959095832298,\n",
       "  (2, 15): 0.49954888759362204,\n",
       "  (2, 16): 0.4926923836454676,\n",
       "  (2, 17): 0.4816593101504405,\n",
       "  (2, 18): 0.43680139544347146,\n",
       "  (2, 19): 0.41273357938461047,\n",
       "  (2, 20): 0.41012882229266623,\n",
       "  (2, 21): 0.39093147702478565,\n",
       "  (2, 22): 0.3886544336037041,\n",
       "  (2, 23): 0.4087668999788034,\n",
       "  (2, 24): 0.44253732273737395,\n",
       "  (3, 4): 0.899354096524149,\n",
       "  (3, 5): 0.818926474284141,\n",
       "  (3, 6): 0.7505414291542799,\n",
       "  (3, 7): 0.704114921014058,\n",
       "  (3, 8): 0.6699693205466755,\n",
       "  (3, 9): 0.6375795736378769,\n",
       "  (3, 10): 0.6034254139689147,\n",
       "  (3, 11): 0.5822271369558657,\n",
       "  (3, 12): 0.5717674911884943,\n",
       "  (3, 13): 0.5584820050259096,\n",
       "  (3, 14): 0.5457000944858184,\n",
       "  (3, 15): 0.528855595674289,\n",
       "  (3, 16): 0.5210153981585394,\n",
       "  (3, 17): 0.5095259013374804,\n",
       "  (3, 18): 0.46646539933567427,\n",
       "  (3, 19): 0.44152231377796697,\n",
       "  (3, 20): 0.4404565532518621,\n",
       "  (3, 21): 0.4259404941183344,\n",
       "  (3, 22): 0.4238714146841085,\n",
       "  (3, 23): 0.44323657493607466,\n",
       "  (3, 24): 0.47109187366916006,\n",
       "  (4, 5): 0.90045150200232,\n",
       "  (4, 6): 0.8183584003993661,\n",
       "  (4, 7): 0.7630207585763937,\n",
       "  (4, 8): 0.7229944774920539,\n",
       "  (4, 9): 0.6858333554789983,\n",
       "  (4, 10): 0.6474695728142825,\n",
       "  (4, 11): 0.622629655856097,\n",
       "  (4, 12): 0.6101945223471144,\n",
       "  (4, 13): 0.5936379854545615,\n",
       "  (4, 14): 0.5777128415727989,\n",
       "  (4, 15): 0.5581656925526685,\n",
       "  (4, 16): 0.5485678301308067,\n",
       "  (4, 17): 0.5362064851209305,\n",
       "  (4, 18): 0.4897454359985964,\n",
       "  (4, 19): 0.4622294030840866,\n",
       "  (4, 20): 0.459878967651076,\n",
       "  (4, 21): 0.44516956869876506,\n",
       "  (4, 22): 0.4426534896104248,\n",
       "  (4, 23): 0.4588914382377655,\n",
       "  (4, 24): 0.48473708313867275,\n",
       "  (5, 6): 0.8993759793046671,\n",
       "  (5, 7): 0.8309547285849691,\n",
       "  (5, 8): 0.7841649634948414,\n",
       "  (5, 9): 0.7418006225265333,\n",
       "  (5, 10): 0.6991462720421548,\n",
       "  (5, 11): 0.6704366233294131,\n",
       "  (5, 12): 0.6557379887741364,\n",
       "  (5, 13): 0.635338185033508,\n",
       "  (5, 14): 0.6156716851090313,\n",
       "  (5, 15): 0.5934068094303266,\n",
       "  (5, 16): 0.581921747593872,\n",
       "  (5, 17): 0.5684466262752105,\n",
       "  (5, 18): 0.5190066175417001,\n",
       "  (5, 19): 0.48934925945885643,\n",
       "  (5, 20): 0.48755411764581424,\n",
       "  (5, 21): 0.47266007620453204,\n",
       "  (5, 22): 0.46941806560065275,\n",
       "  (5, 23): 0.4845253564258571,\n",
       "  (5, 24): 0.502543232653795,\n",
       "  (6, 7): 0.9108840511834373,\n",
       "  (6, 8): 0.8540166651222768,\n",
       "  (6, 9): 0.8049662372661844,\n",
       "  (6, 10): 0.7567496673739441,\n",
       "  (6, 11): 0.7238887574322019,\n",
       "  (6, 12): 0.7066218029744001,\n",
       "  (6, 13): 0.6816270049821559,\n",
       "  (6, 14): 0.6580053794455681,\n",
       "  (6, 15): 0.6324607913440347,\n",
       "  (6, 16): 0.6189984470450667,\n",
       "  (6, 17): 0.6041855038976645,\n",
       "  (6, 18): 0.5504055947287437,\n",
       "  (6, 19): 0.5180172502795886,\n",
       "  (6, 20): 0.5150600931525668,\n",
       "  (6, 21): 0.4980228815944019,\n",
       "  (6, 22): 0.4941760712992543,\n",
       "  (6, 23): 0.50905104426512,\n",
       "  (6, 24): 0.5201006283730166,\n",
       "  (7, 8): 0.9305874233558266,\n",
       "  (7, 9): 0.8756473043774488,\n",
       "  (7, 10): 0.8226643086233058,\n",
       "  (7, 11): 0.7849609528988788,\n",
       "  (7, 12): 0.7640978661443487,\n",
       "  (7, 13): 0.7346917292730653,\n",
       "  (7, 14): 0.7063787043269208,\n",
       "  (7, 15): 0.6779050056959436,\n",
       "  (7, 16): 0.6609293075409867,\n",
       "  (7, 17): 0.6444204838055517,\n",
       "  (7, 18): 0.5851940164104596,\n",
       "  (7, 19): 0.5487761622064503,\n",
       "  (7, 20): 0.5439484607233062,\n",
       "  (7, 21): 0.5226447263580525,\n",
       "  (7, 22): 0.5174826929808671,\n",
       "  (7, 23): 0.5299710228946053,\n",
       "  (7, 24): 0.5325353470319661,\n",
       "  (8, 9): 0.9343008873613766,\n",
       "  (8, 10): 0.8759928614189123,\n",
       "  (8, 11): 0.8346583640999702,\n",
       "  (8, 12): 0.8113989718916113,\n",
       "  (8, 13): 0.777956676700024,\n",
       "  (8, 14): 0.7461352124159769,\n",
       "  (8, 15): 0.7152858840807262,\n",
       "  (8, 16): 0.695715321519926,\n",
       "  (8, 17): 0.6776949597275764,\n",
       "  (8, 18): 0.6153419195761473,\n",
       "  (8, 19): 0.575704959584951,\n",
       "  (8, 20): 0.5683403863563685,\n",
       "  (8, 21): 0.5440508253610855,\n",
       "  (8, 22): 0.5376828078063617,\n",
       "  (8, 23): 0.5496867267892628,\n",
       "  (8, 24): 0.5432349602893033,\n",
       "  (9, 10): 0.9333234311746667,\n",
       "  (9, 11): 0.8863987289246151,\n",
       "  (9, 12): 0.8601285889000051,\n",
       "  (9, 13): 0.82216174897805,\n",
       "  (9, 14): 0.786042106457022,\n",
       "  (9, 15): 0.7529312548485189,\n",
       "  (9, 16): 0.7307965130028009,\n",
       "  (9, 17): 0.7112989913634679,\n",
       "  (9, 18): 0.6448939641767262,\n",
       "  (9, 19): 0.6018129538818758,\n",
       "  (9, 20): 0.5910867735325844,\n",
       "  (9, 21): 0.5634279926137503,\n",
       "  (9, 22): 0.555408947260287,\n",
       "  (9, 23): 0.5647878771492031,\n",
       "  (9, 24): 0.5518377038801601,\n",
       "  (10, 11): 0.9455222714120035,\n",
       "  (10, 12): 0.9160923716194657,\n",
       "  (10, 13): 0.8749128969062255,\n",
       "  (10, 14): 0.8362929984206872,\n",
       "  (10, 15): 0.8011629660070367,\n",
       "  (10, 16): 0.7765881636400316,\n",
       "  (10, 17): 0.7550582578712767,\n",
       "  (10, 18): 0.6887783229077069,\n",
       "  (10, 19): 0.6424713939156597,\n",
       "  (10, 20): 0.6283756175967737,\n",
       "  (10, 21): 0.5972273309711221,\n",
       "  (10, 22): 0.5862648771752353,\n",
       "  (10, 23): 0.5923286669100952,\n",
       "  (10, 24): 0.5688962778483165,\n",
       "  (11, 12): 0.9652728139259749,\n",
       "  (11, 13): 0.9190126360903434,\n",
       "  (11, 14): 0.8773467993460136,\n",
       "  (11, 15): 0.839672096678135,\n",
       "  (11, 16): 0.8138424268051673,\n",
       "  (11, 17): 0.7907003771019202,\n",
       "  (11, 18): 0.7205606790579323,\n",
       "  (11, 19): 0.671764147637107,\n",
       "  (11, 20): 0.6516262568329596,\n",
       "  (11, 21): 0.6188813160377303,\n",
       "  (11, 22): 0.606079320487181,\n",
       "  (11, 23): 0.611507924586914,\n",
       "  (11, 24): 0.5808067440798533,\n",
       "  (12, 13): 0.9481889384146633,\n",
       "  (12, 14): 0.9042129559232622,\n",
       "  (12, 15): 0.864334732525727,\n",
       "  (12, 16): 0.8375695959397342,\n",
       "  (12, 17): 0.8141909634217063,\n",
       "  (12, 18): 0.7418260270248793,\n",
       "  (12, 19): 0.6916436855974745,\n",
       "  (12, 20): 0.6686650117028344,\n",
       "  (12, 21): 0.6342985329125236,\n",
       "  (12, 22): 0.6212488730415687,\n",
       "  (12, 23): 0.6261806082677014,\n",
       "  (12, 24): 0.5888884116106301,\n",
       "  (13, 14): 0.9502008462620987,\n",
       "  (13, 15): 0.9073111626155498,\n",
       "  (13, 16): 0.8792889578675123,\n",
       "  (13, 17): 0.8526134139873879,\n",
       "  (13, 18): 0.7788153446822647,\n",
       "  (13, 19): 0.726881493927115,\n",
       "  (13, 20): 0.6984354769397297,\n",
       "  (13, 21): 0.6639009146790109,\n",
       "  (13, 22): 0.649388375844696,\n",
       "  (13, 23): 0.6576738000099153,\n",
       "  (13, 24): 0.6103566198298169,\n",
       "  (14, 15): 0.9503377651228505,\n",
       "  (14, 16): 0.9193531433132334,\n",
       "  (14, 17): 0.8900568323778147,\n",
       "  (14, 18): 0.8192193578540442,\n",
       "  (14, 19): 0.7643881853977694,\n",
       "  (14, 20): 0.731464185000393,\n",
       "  (14, 21): 0.6966904506646122,\n",
       "  (14, 22): 0.6805011515668745,\n",
       "  (14, 23): 0.6885845258072845,\n",
       "  (14, 24): 0.6298350219192481,\n",
       "  (15, 16): 0.9632664767536165,\n",
       "  (15, 17): 0.9303964642174595,\n",
       "  (15, 18): 0.8547099217377205,\n",
       "  (15, 19): 0.7969864005046222,\n",
       "  (15, 20): 0.7589241072756329,\n",
       "  (15, 21): 0.7195457382713816,\n",
       "  (15, 22): 0.7009685363218866,\n",
       "  (15, 23): 0.7078999613202227,\n",
       "  (15, 24): 0.6436493917096345,\n",
       "  (16, 17): 0.9629414365752492,\n",
       "  (16, 18): 0.8807389561701203,\n",
       "  (16, 19): 0.8205148531922171,\n",
       "  (16, 20): 0.776830823907717,\n",
       "  (16, 21): 0.7352428191069075,\n",
       "  (16, 22): 0.7156929300445909,\n",
       "  (16, 23): 0.7256976157107505,\n",
       "  (16, 24): 0.6542813424330277,\n",
       "  (17, 18): 0.9083808341147946,\n",
       "  (17, 19): 0.8436247289762345,\n",
       "  (17, 20): 0.7933556661793549,\n",
       "  (17, 21): 0.7490027213342078,\n",
       "  (17, 22): 0.7283944458704692,\n",
       "  (17, 23): 0.7371351575836173,\n",
       "  (17, 24): 0.66140525512949,\n",
       "  (18, 19): 0.9224083348672801,\n",
       "  (18, 20): 0.8636724609140813,\n",
       "  (18, 21): 0.8245936613344647,\n",
       "  (18, 22): 0.7989761439370758,\n",
       "  (18, 23): 0.7952084870999866,\n",
       "  (18, 24): 0.6923306233520595,\n",
       "  (19, 20): 0.9148196052602257,\n",
       "  (19, 21): 0.8654330961246481,\n",
       "  (19, 22): 0.833887953701255,\n",
       "  (19, 23): 0.8214735590166639,\n",
       "  (19, 24): 0.7089472573292905,\n",
       "  (20, 21): 0.931040396692684,\n",
       "  (20, 22): 0.8903660491814436,\n",
       "  (20, 23): 0.8634275484557973,\n",
       "  (20, 24): 0.7362594957115204,\n",
       "  (21, 22): 0.9501695505637248,\n",
       "  (21, 23): 0.925291088038584,\n",
       "  (21, 24): 0.795087725057145,\n",
       "  (22, 23): 0.9723578243082017,\n",
       "  (22, 24): 0.8383927253325147,\n",
       "  (23, 24): 0.8657944741174436},\n",
       " 'score_id': {(0, 1): 0.8467146857976031,\n",
       "  (0, 2): 0.7114601287824575,\n",
       "  (0, 3): 0.5969254528644591,\n",
       "  (0, 4): 0.4852890041378423,\n",
       "  (0, 5): 0.3843512614884173,\n",
       "  (0, 6): 0.30934095668912637,\n",
       "  (0, 7): 0.2552487156036541,\n",
       "  (0, 8): 0.21715553939254106,\n",
       "  (0, 9): 0.1889681767384236,\n",
       "  (0, 10): 0.1524877085476627,\n",
       "  (0, 11): 0.11733583434505908,\n",
       "  (0, 12): 0.10218128832102391,\n",
       "  (0, 13): 0.06255454867890195,\n",
       "  (0, 14): 0.030482002640157973,\n",
       "  (0, 15): 0.0024175642531727762,\n",
       "  (0, 16): -0.013136491045785658,\n",
       "  (0, 17): -0.005536080659567157,\n",
       "  (0, 18): -0.04330007381325029,\n",
       "  (0, 19): -0.04731169648129247,\n",
       "  (0, 20): -0.08266727836289599,\n",
       "  (0, 21): -0.1020576804425737,\n",
       "  (0, 22): -0.1189700500916212,\n",
       "  (0, 23): -0.10781144878667259,\n",
       "  (0, 24): -13.921909645759209,\n",
       "  (1, 2): 0.8509154643085556,\n",
       "  (1, 3): 0.7204070966939825,\n",
       "  (1, 4): 0.5916495735384713,\n",
       "  (1, 5): 0.4735700354241013,\n",
       "  (1, 6): 0.38207616072036804,\n",
       "  (1, 7): 0.3132230224981576,\n",
       "  (1, 8): 0.26962819513031455,\n",
       "  (1, 9): 0.23646120111863098,\n",
       "  (1, 10): 0.19477498171690555,\n",
       "  (1, 11): 0.1576823714580024,\n",
       "  (1, 12): 0.1405200315610526,\n",
       "  (1, 13): 0.09776972674570733,\n",
       "  (1, 14): 0.06353498085880949,\n",
       "  (1, 15): 0.03235464311361029,\n",
       "  (1, 16): 0.01483528353182365,\n",
       "  (1, 17): 0.019662722255626427,\n",
       "  (1, 18): -0.02405066048153177,\n",
       "  (1, 19): -0.03267484171572395,\n",
       "  (1, 20): -0.07427711870701044,\n",
       "  (1, 21): -0.09632207402797677,\n",
       "  (1, 22): -0.11534176001800774,\n",
       "  (1, 23): -0.10651035593131071,\n",
       "  (1, 24): -14.052150689610313,\n",
       "  (2, 3): 0.8548716910320934,\n",
       "  (2, 4): 0.7082753219259995,\n",
       "  (2, 5): 0.5780754375688631,\n",
       "  (2, 6): 0.4646059001718262,\n",
       "  (2, 7): 0.37920420676060074,\n",
       "  (2, 8): 0.3270536442446536,\n",
       "  (2, 9): 0.28600363720765704,\n",
       "  (2, 10): 0.23908145149701832,\n",
       "  (2, 11): 0.1994511823644928,\n",
       "  (2, 12): 0.18094611130370808,\n",
       "  (2, 13): 0.13635197377032052,\n",
       "  (2, 14): 0.09976604061399681,\n",
       "  (2, 15): 0.065376783749621,\n",
       "  (2, 16): 0.04527522657391986,\n",
       "  (2, 17): 0.04755417265689737,\n",
       "  (2, 18): -0.0008480934274186525,\n",
       "  (2, 19): -0.01421643739059845,\n",
       "  (2, 20): -0.06254928467976116,\n",
       "  (2, 21): -0.08697336115960423,\n",
       "  (2, 22): -0.10872676491386722,\n",
       "  (2, 23): -0.10401509500337426,\n",
       "  (2, 24): -14.434513879211528,\n",
       "  (3, 4): 0.8306102967808441,\n",
       "  (3, 5): 0.6829963604233992,\n",
       "  (3, 6): 0.550125396802767,\n",
       "  (3, 7): 0.4485916824422625,\n",
       "  (3, 8): 0.3879389330273361,\n",
       "  (3, 9): 0.3388394937135298,\n",
       "  (3, 10): 0.2863641824657817,\n",
       "  (3, 11): 0.24386044349411282,\n",
       "  (3, 12): 0.22299669359090643,\n",
       "  (3, 13): 0.17515463149761587,\n",
       "  (3, 14): 0.13599553066581085,\n",
       "  (3, 15): 0.09831542715239132,\n",
       "  (3, 16): 0.07583970146300227,\n",
       "  (3, 17): 0.07493531653942918,\n",
       "  (3, 18): 0.02152608547536851,\n",
       "  (3, 19): 0.002954815998296697,\n",
       "  (3, 20): -0.05175030909282894,\n",
       "  (3, 21): -0.07913505304166801,\n",
       "  (3, 22): -0.10350458956555791,\n",
       "  (3, 23): -0.10209885247161617,\n",
       "  (3, 24): -14.607267165206295,\n",
       "  (4, 5): 0.8219656311210994,\n",
       "  (4, 6): 0.6652131670171865,\n",
       "  (4, 7): 0.5448439278233145,\n",
       "  (4, 8): 0.4718769667223179,\n",
       "  (4, 9): 0.41166729171207495,\n",
       "  (4, 10): 0.3507814888788346,\n",
       "  (4, 11): 0.30302634895431196,\n",
       "  (4, 12): 0.27860845182886146,\n",
       "  (4, 13): 0.22552759683277349,\n",
       "  (4, 14): 0.1813573307666232,\n",
       "  (4, 15): 0.13849694395033849,\n",
       "  (4, 16): 0.11292366991698344,\n",
       "  (4, 17): 0.10839573661465174,\n",
       "  (4, 18): 0.04787603820871783,\n",
       "  (4, 19): 0.0232403611988109,\n",
       "  (4, 20): -0.039050664760719514,\n",
       "  (4, 21): -0.07003404300146122,\n",
       "  (4, 22): -0.09761416492604484,\n",
       "  (4, 23): -0.09992397825080428,\n",
       "  (4, 24): -14.783928284435348,\n",
       "  (5, 6): 0.8099822377833755,\n",
       "  (5, 7): 0.6660865344400942,\n",
       "  (5, 8): 0.5784050437792448,\n",
       "  (5, 9): 0.5054196938045529,\n",
       "  (5, 10): 0.43434649514718754,\n",
       "  (5, 11): 0.3791738368223423,\n",
       "  (5, 12): 0.3505341237945001,\n",
       "  (5, 13): 0.2905006879601813,\n",
       "  (5, 14): 0.23983329734640843,\n",
       "  (5, 15): 0.19049666053026448,\n",
       "  (5, 16): 0.16059301414089353,\n",
       "  (5, 17): 0.15168125776910515,\n",
       "  (5, 18): 0.08235483877877513,\n",
       "  (5, 19): 0.04988641835667951,\n",
       "  (5, 20): -0.021893461386661633,\n",
       "  (5, 21): -0.05732681231977523,\n",
       "  (5, 22): -0.08921802093831917,\n",
       "  (5, 23): -0.09676206209793774,\n",
       "  (5, 24): -15.586727470481227,\n",
       "  (6, 7): 0.8155781752995009,\n",
       "  (6, 8): 0.7077660793271193,\n",
       "  (6, 9): 0.6190392163491335,\n",
       "  (6, 10): 0.5351741698424843,\n",
       "  (6, 11): 0.47062781942417226,\n",
       "  (6, 12): 0.4360773453978326,\n",
       "  (6, 13): 0.3657075309745383,\n",
       "  (6, 14): 0.3075413679827182,\n",
       "  (6, 15): 0.25041700521896776,\n",
       "  (6, 16): 0.2152417393048608,\n",
       "  (6, 17): 0.20089694115885057,\n",
       "  (6, 18): 0.11953769831553457,\n",
       "  (6, 19): 0.07758187231714174,\n",
       "  (6, 20): -0.005195743482277305,\n",
       "  (6, 21): -0.046340887864226084,\n",
       "  (6, 22): -0.08245999217528294,\n",
       "  (6, 23): -0.09451929938006763,\n",
       "  (6, 24): -16.15820067262879,\n",
       "  (7, 8): 0.8643113525438008,\n",
       "  (7, 9): 0.7596672072042101,\n",
       "  (7, 10): 0.6645916583710335,\n",
       "  (7, 11): 0.5890220541640753,\n",
       "  (7, 12): 0.5455784892784593,\n",
       "  (7, 13): 0.4654119943550792,\n",
       "  (7, 14): 0.3993353132769643,\n",
       "  (7, 15): 0.3333196969867358,\n",
       "  (7, 16): 0.2913170866264107,\n",
       "  (7, 17): 0.26902428294857067,\n",
       "  (7, 18): 0.1729872168796337,\n",
       "  (7, 19): 0.11830971218198534,\n",
       "  (7, 20): 0.02024495960536256,\n",
       "  (7, 21): -0.028287884659375664,\n",
       "  (7, 22): -0.07058381964599557,\n",
       "  (7, 23): -0.0900574499629347,\n",
       "  (7, 24): -18.110546528449206,\n",
       "  (8, 9): 0.8751417729073069,\n",
       "  (8, 10): 0.7672531839396269,\n",
       "  (8, 11): 0.6848749559393168,\n",
       "  (8, 12): 0.6354987224488222,\n",
       "  (8, 13): 0.5473118012216476,\n",
       "  (8, 14): 0.474235143239602,\n",
       "  (8, 15): 0.40040685148992333,\n",
       "  (8, 16): 0.35202038083697706,\n",
       "  (8, 17): 0.324358954798931,\n",
       "  (8, 18): 0.21630657699670627,\n",
       "  (8, 19): 0.1516559967472995,\n",
       "  (8, 20): 0.040438949206350494,\n",
       "  (8, 21): -0.014552361372956462,\n",
       "  (8, 22): -0.061780908132898917,\n",
       "  (8, 23): -0.0870829073858153,\n",
       "  (8, 24): -18.766863577880496,\n",
       "  (9, 10): 0.8717656608190287,\n",
       "  (9, 11): 0.7794626651375745,\n",
       "  (9, 12): 0.7237480637719348,\n",
       "  (9, 13): 0.6277397063102738,\n",
       "  (9, 14): 0.5471452609576769,\n",
       "  (9, 15): 0.4668560188387039,\n",
       "  (9, 16): 0.41273050539019057,\n",
       "  (9, 17): 0.37941156610415866,\n",
       "  (9, 18): 0.2596668972297001,\n",
       "  (9, 19): 0.1852713921557001,\n",
       "  (9, 20): 0.0606153024092602,\n",
       "  (9, 21): -0.0011387389267465016,\n",
       "  (9, 22): -0.053467117796412555,\n",
       "  (9, 23): -0.08425898693646133,\n",
       "  (9, 24): -20.281047790515693,\n",
       "  (10, 11): 0.8899469643915005,\n",
       "  (10, 12): 0.8265791797968598,\n",
       "  (10, 13): 0.7243430222800096,\n",
       "  (10, 14): 0.6381288197796479,\n",
       "  (10, 15): 0.5531443082368368,\n",
       "  (10, 16): 0.49267799208881763,\n",
       "  (10, 17): 0.45163036051437305,\n",
       "  (10, 18): 0.32252313568107444,\n",
       "  (10, 19): 0.2358091090935759,\n",
       "  (10, 20): 0.09272421648605432,\n",
       "  (10, 21): 0.0207560404346145,\n",
       "  (10, 22): -0.0392031187911054,\n",
       "  (10, 23): -0.07909605472544975,\n",
       "  (10, 24): -21.874801261100384,\n",
       "  (11, 12): 0.9217520443247107,\n",
       "  (11, 13): 0.8125826634139096,\n",
       "  (11, 14): 0.7211567410174711,\n",
       "  (11, 15): 0.6315614259090815,\n",
       "  (11, 16): 0.5663854240283096,\n",
       "  (11, 17): 0.5164425434304589,\n",
       "  (11, 18): 0.3759619101368708,\n",
       "  (11, 19): 0.2780433583910087,\n",
       "  (11, 20): 0.11643280657775358,\n",
       "  (11, 21): 0.03681296768510491,\n",
       "  (11, 22): -0.029073179490064733,\n",
       "  (11, 23): -0.07576313131292356,\n",
       "  (11, 24): -23.444380640604496,\n",
       "  (12, 13): 0.8766719704513687,\n",
       "  (12, 14): 0.7796804830049925,\n",
       "  (12, 15): 0.6821165452291402,\n",
       "  (12, 16): 0.6142278023309694,\n",
       "  (12, 17): 0.5615562045468674,\n",
       "  (12, 18): 0.41266446507275706,\n",
       "  (12, 19): 0.3066315061038024,\n",
       "  (12, 20): 0.13532230872818507,\n",
       "  (12, 21): 0.049383637740344925,\n",
       "  (12, 22): -0.0206269083723845,\n",
       "  (12, 23): -0.07241467019021454,\n",
       "  (12, 24): -24.213207242909714,\n",
       "  (13, 14): 0.8874749604031321,\n",
       "  (13, 15): 0.7828542069097911,\n",
       "  (13, 16): 0.7102097493612737,\n",
       "  (13, 17): 0.649211073657953,\n",
       "  (13, 18): 0.4910714110654717,\n",
       "  (13, 19): 0.3709199706641841,\n",
       "  (13, 20): 0.17799857040427483,\n",
       "  (13, 21): 0.08004689023437139,\n",
       "  (13, 22): 0.001253128226496471,\n",
       "  (13, 23): -0.06439932118459434,\n",
       "  (13, 24): -25.793741962776817,\n",
       "  (14, 15): 0.8776414883119692,\n",
       "  (14, 16): 0.7974972650401977,\n",
       "  (14, 17): 0.7246459977046107,\n",
       "  (14, 18): 0.5627035594785906,\n",
       "  (14, 19): 0.4314512626415914,\n",
       "  (14, 20): 0.217601586071551,\n",
       "  (14, 21): 0.10826312477939336,\n",
       "  (14, 22): 0.019814738545636774,\n",
       "  (14, 23): -0.057230345715502486,\n",
       "  (14, 24): -27.96503633356043,\n",
       "  (15, 16): 0.9034143686079442,\n",
       "  (15, 17): 0.8140282445540624,\n",
       "  (15, 18): 0.6431036711837168,\n",
       "  (15, 19): 0.5006528219944164,\n",
       "  (15, 20): 0.26033201630683894,\n",
       "  (15, 21): 0.13722299977031818,\n",
       "  (15, 22): 0.0386970182550492,\n",
       "  (15, 23): -0.05046182815138286,\n",
       "  (15, 24): -33.412055902951735,\n",
       "  (16, 17): 0.8873540602919746,\n",
       "  (16, 18): 0.701955913208853,\n",
       "  (16, 19): 0.5508022894071245,\n",
       "  (16, 20): 0.2961137881039457,\n",
       "  (16, 21): 0.16152426771961467,\n",
       "  (16, 22): 0.05402796950146946,\n",
       "  (16, 23): -0.044290010261241075,\n",
       "  (16, 24): -35.55944914025972,\n",
       "  (17, 18): 0.7837214991434575,\n",
       "  (17, 19): 0.6158014375803426,\n",
       "  (17, 20): 0.3399698188783408,\n",
       "  (17, 21): 0.19215646757435767,\n",
       "  (17, 22): 0.07705624778219768,\n",
       "  (17, 23): -0.03596445445210291,\n",
       "  (17, 24): -38.53232006940305,\n",
       "  (18, 19): 0.7989595752583049,\n",
       "  (18, 20): 0.48171284591494573,\n",
       "  (18, 21): 0.3052078384257814,\n",
       "  (18, 22): 0.1604514575177535,\n",
       "  (18, 23): 9.197545276354187e-05,\n",
       "  (18, 24): -62.20234645288647,\n",
       "  (19, 20): 0.6310366439183566,\n",
       "  (19, 21): 0.42145166968061387,\n",
       "  (19, 22): 0.24386850495252366,\n",
       "  (19, 23): 0.03593735788077996,\n",
       "  (19, 24): -90.36810603713091,\n",
       "  (20, 21): 0.7122605872372461,\n",
       "  (20, 22): 0.4679566651690827,\n",
       "  (20, 23): 0.14078415707807515,\n",
       "  (20, 24): -4773.626681523641,\n",
       "  (21, 22): 0.7155532341504717,\n",
       "  (21, 23): 0.2619201685472779,\n",
       "  (21, 24): -43626.05235520549,\n",
       "  (22, 23): 0.43790170788628974,\n",
       "  (22, 24): -106171.50833291434,\n",
       "  (23, 24): -65039.33716087104}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a86e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the r2_score's key\n",
    "connections = list(r2_scores['score_mat'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c02770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909bb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LayerSelector \n",
    ": select layers included in forwarding.\n",
    "n is the number of selected layers.\n",
    "0th and 24th layers are always included, So we need to select intermediate layers(n-2).\n",
    "\n",
    "I implemented this LayerSelector class that makes all possible selected layer list now.\n",
    "-> this implementation requires too much resources.\n",
    "-> todo : Change this process with an algorithm\n",
    "\n",
    "\"\"\"\n",
    "class LayerSelector:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.selected_layers = []\n",
    "        self.arch = []\n",
    "\n",
    "    def selecting(self, prev, num_selected):\n",
    "        if num_selected == self.n - 2:\n",
    "            self.selected_layers.append(24)\n",
    "            self.selected_layers.insert(0, 0)\n",
    "            self.arch.append(self.selected_layers[:])\n",
    "            self.selected_layers.pop()\n",
    "            self.selected_layers.pop(0)\n",
    "            return\n",
    "\n",
    "        for i in range(prev + 1, 24):\n",
    "            self.selected_layers.append(i)\n",
    "            self.selecting(i, num_selected + 1)\n",
    "            self.selected_layers.pop()\n",
    "\n",
    "    def select_layer(self):\n",
    "        self.selecting(0, 0)\n",
    "        return self.arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9972acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 24], [0, 2, 24], [0, 3, 24], [0, 4, 24], [0, 5, 24], [0, 6, 24], [0, 7, 24], [0, 8, 24], [0, 9, 24], [0, 10, 24], [0, 11, 24], [0, 12, 24], [0, 13, 24], [0, 14, 24], [0, 15, 24], [0, 16, 24], [0, 17, 24], [0, 18, 24], [0, 19, 24], [0, 20, 24], [0, 21, 24], [0, 22, 24], [0, 23, 24]]\n"
     ]
    }
   ],
   "source": [
    "selector = LayerSelector(3)\n",
    "layers = selector.select_layer()\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "481fcf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load_arch\n",
    ": this function takes selected layer numbers and loads real shorcut weights from the shorcut model.\n",
    "\n",
    "linear_layers are list of the real weights.\n",
    "'''\n",
    "def load_arch(selected_layers):\n",
    "    base_path = \"linreg/mamba-130m-hf/xsum\"\n",
    "    linear_layers = []\n",
    "    for i in range(1, len(selected_layers)):\n",
    "        name = f\"{selected_layers[i-1]}_{selected_layers[i]}.pickle\"\n",
    "        full_path = os.path.join(base_path, name)\n",
    "        with open(full_path, 'rb') as file:\n",
    "            linreg = pickle.load(file)\n",
    "            linear_layers.append(linreg)\n",
    "    return linear_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c260459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 24]\n"
     ]
    }
   ],
   "source": [
    "print(layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b8bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n",
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "linear_layers = load_arch(layers[0])\n",
    "for layer in linear_layers:\n",
    "    print(layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d694ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tried to write beam search code\n",
    "I was struggling because huggingface's mamba code doesn't get last state in forward code.\n",
    "If I modify and use customized mamba code then I think we can't use finetuned weights.\n",
    "What should I do?\n",
    "\"\"\"\n",
    "\n",
    "LENGTH_PENALTY = 1.2\n",
    "MIN_LENGTH = 5\n",
    "\n",
    "class SingleBeamSearchSpace():\n",
    "\n",
    "    def __init__(self, decoder_input, beam_size, max_length = 255):\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        super(SingleBeamSearchSpace, self).__init__()\n",
    "\n",
    "        self.device = decoder_input['input_ids'].device\n",
    "        self.word_indice = [torch.LongTensor(self.beam_size).zero_().to(self.device)]\n",
    "        self.prev_beam_indice = [torch.LongTensor(self.beam_size).zero_().to(self.device) - 1]\n",
    "        self.cumulative_probs = [torch.FloatTensor([.0] + [-float('inf')] * (beam_size - 1)).to(self.device)]\n",
    "        self.masks = [torch.ByteTensor(beam_size).zero_().to(self.device)] # 1 if it is done else 0\n",
    "        self.decoder_input = decoder_input\n",
    "\n",
    "        self.decoder_input_ids = self.decoder_input['input_ids'].repeat(self.beam_size, 1)\n",
    "        self.decoder_attention_masks = self.decoder_input['attention_mask'].repeat(self.beam_size, 1)\n",
    "\n",
    "        self.current_time_step = 0\n",
    "        self.done_cnt = 0\n",
    "\n",
    "    def get_length_penalty(self, length, alpha = LENGTH_PENALTY, min_length = MIN_LENGTH):\n",
    "        p = (1 + length) ** alpha / (1 + min_length) ** alpha\n",
    "\n",
    "        return p\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.done_cnt >= self.beam_size:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def get_batch(self):\n",
    "        y_prev = self.word_indice[-1].unsqueeze(-1)\n",
    "        \n",
    "        return y_prev\n",
    "\n",
    "    def collect_result(self, y_hat):\n",
    "        output_size = y_hat.size(-1)\n",
    "\n",
    "        self.current_time_step += 1\n",
    "\n",
    "        cumulative_prob = y_hat + self.cumulative_probs[-1].masked_fill_(self.masks[-1], -float('inf')).view(-1, 1, 1).expand(self.beam_size, 1, output_size)\n",
    "        top_log_prob, top_indice = torch.topk(cumulative_prob.view(-1), self.beam_size, dim = -1)\n",
    "        # |top_log_prob| = (beam_size)\n",
    "        # |top_indice| = (beam_size)\n",
    "        self.word_indice += [top_indice.fmod(output_size)]\n",
    "        self.prev_beam_indice += [top_indice.div(output_size).long()]\n",
    "\n",
    "        self.cumulative_probs += [top_log_prob]\n",
    "        self.masks += [torch.eq(self.word_indice[-1], tokenizer.eos_token_id)]\n",
    "        self.done_cnt += self.masks[-1].float().sum()\n",
    "\n",
    "        #self.prev_state = torch.index_select(prev_state, dim = 1, index = self.prev_beam_indice[-1]).contiguous()\n",
    "\n",
    "    def get_n_best(self, n = 1):\n",
    "        sentences = []\n",
    "        probs = []\n",
    "        founds = []\n",
    "\n",
    "        for t in range(len(self.word_indice)):\n",
    "            for b in range(self.beam_size):\n",
    "                if self.masks[t][b] == 1:\n",
    "                    probs += [self.cumulative_probs[t][b] / self.get_length_penalty(t)]\n",
    "                    founds += [(t, b)]\n",
    "\n",
    "        for b in range(self.beam_size):\n",
    "            if self.cumulative_probs[-1][b] != -float('inf'):\n",
    "                if not (len(self.cumulative_probs) - 1, b) in founds:\n",
    "                    probs += [self.cumulative_probs[-1][b]]\n",
    "                    founds += [(t, b)]\n",
    "\n",
    "        sorted_founds_with_probs = sorted(zip(founds, probs), \n",
    "                                            key = itemgetter(1), \n",
    "                                            reverse = True\n",
    "                                            )[:n]\n",
    "        probs = []\n",
    "\n",
    "        for (end_index, b), prob in sorted_founds_with_probs:\n",
    "            sentence = []\n",
    "\n",
    "            for t in range(end_index, 0, -1):\n",
    "                sentence = [self.word_indice[t][b]] + sentence\n",
    "                b = self.prev_beam_indice[t][b]\n",
    "\n",
    "            sentences += [sentence]\n",
    "            probs += [prob]\n",
    "\n",
    "        return sentences, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee97dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_beam_search(model, tokenized_input, beam_size, max_length = 255, n_best = 1):\n",
    "    model.eval()\n",
    "    x = tokenized_input['input_ids']\n",
    "    input_mask = tokenized_input['attention_mask']\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    # initialize beam-search.\n",
    "    spaces = [SingleBeamSearchSpace(tokenized_input, \n",
    "                                        beam_size=beam_size\n",
    "                                        ) for i in range(batch_size)]\n",
    "    done_cnt = [space.is_done() for space in spaces]\n",
    "\n",
    "    length = 0\n",
    "    while sum(done_cnt) < batch_size and length <= max_length:\n",
    "        # current_batch_size = sum(done_cnt) * beam_size\n",
    "\n",
    "        # initialize fabricated variables.\n",
    "        fab_input = []\n",
    "        fab_h_src, fab_mask = [], []\n",
    "\n",
    "        # batchify.\n",
    "        for i, space in enumerate(spaces):\n",
    "            if space.is_done() == 0:\n",
    "                y_prev_ = space.get_batch()\n",
    "\n",
    "                fab_input += [y_prev_]\n",
    "\n",
    "                fab_h_src += [x[i, :]] * beam_size\n",
    "                fab_mask += [input_mask[i, :]] * beam_size\n",
    "\n",
    "        fab_input = torch.cat(fab_input, dim = 0)\n",
    "        fab_h_src = torch.stack(fab_h_src)\n",
    "\n",
    "        fab_output_logits = model(fab_input).logits[:, -1, :]\n",
    "        \n",
    "        y_hat = torch.log_softmax(fab_output_logits, dim=-1)\n",
    "        output_size = y_hat.shape[-1]\n",
    "\n",
    "        cnt = 0\n",
    "        for space in spaces:\n",
    "            if space.is_done() == 0:\n",
    "                from_index = cnt * output_size\n",
    "                to_index = (cnt + 1) * output_size\n",
    "\n",
    "                # pick k-best results for each sample.\n",
    "                space.collect_result(y_hat[from_index:to_index])\n",
    "                cnt += 1\n",
    "\n",
    "        done_cnt = [space.is_done() for space in spaces]\n",
    "        length += 1\n",
    "\n",
    "    batch_sentences = []\n",
    "    batch_probs = []\n",
    "\n",
    "    for i, space in enumerate(spaces):\n",
    "        sentences, probs = space.get_n_best(n_best)\n",
    "\n",
    "        batch_sentences += [sentences]\n",
    "        batch_probs += [probs]\n",
    "\n",
    "    return batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3d1ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortcutModel(nn.Module):\n",
    "    def __init__(self, n, selected_layers):\n",
    "        super(ShortcutModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.path = nn.ModuleList([nn.Linear(768, 768) for _ in range(self.n-1)])\n",
    "        self.selected_layers = selected_layers\n",
    "        self.weight_list = []\n",
    "        self.tokenizer = get_tokenizer('state-spaces/mamba-130m-hf')\n",
    "        self.basemodel = get_model('state-spaces/mamba-130m-hf')\n",
    "        self.embed = basemodel.backbone.embeddings\n",
    "        self.norm = basemodel.backbone.norm_f\n",
    "        self.lm_head = basemodel.lm_head\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        base_path = \"linreg/mamba-130m-hf/xsum\"\n",
    "        for i in range(1, self.n):\n",
    "            name = f\"{self.selected_layers[i-1]}_{self.selected_layers[i]}.pickle\"\n",
    "            full_path = os.path.join(base_path, name)\n",
    "            with open(full_path, 'rb') as file:\n",
    "                linreg = pickle.load(file)\n",
    "                self.weight_list.append(linreg)\n",
    "    \n",
    "    \n",
    "    def initialize(self):\n",
    "        for i in range(self.n-1):\n",
    "            self.path[i].weight.data = self.weight_list[i]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for layer in self.path:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.lm_head(x)\n",
    " \n",
    "        return x\n",
    "\n",
    "    def original(self, x):\n",
    "        x = self.basemodel(x)\n",
    "        return x\n",
    "    \n",
    "#     def decode(self, x):\n",
    "#         result_list = self.result.tolist()\n",
    "#         self.decoded_result = self.tokenizer.decode(result_list, skip_special_tokens=True).split(suffix)[1]\n",
    "#         self.decoded_ref = self.tokenizer.decode(x, skip_special_tokens=True).split(suffix)[1]\n",
    "#         print(self.decoded_result)\n",
    "#         print(self.decoded_ref)\n",
    "        \n",
    "    def get_score(self, x):\n",
    "        rouge = evaluate.load('rouge')\n",
    "        scores = rouge.compute(predictions=self.result, references=x)\n",
    "        print(scores['rouge2'])\n",
    "        return scores['rouge2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d62d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdfd878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = ShortcutModel(3, layers[0])\n",
    "# model.load_weights()\n",
    "# model.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072347ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 50280])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'temperature_scaled_sampling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text[\u001b[38;5;241m0\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#output_tokens = beam_search(model, input_ids, beam_width=3, max_length=50)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_text)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_input_ids, max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m---> 12\u001b[0m next_token_id \u001b[38;5;241m=\u001b[39m \u001b[43mtemperature_scaled_sampling\u001b[49m(logits)\n\u001b[1;32m     13\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_ids, next_token_id], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_token_id\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temperature_scaled_sampling' is not defined"
     ]
    }
   ],
   "source": [
    "#input_text = \"Ferrari appeared in a position to challenge until the final laps, when the Mercedes stretched their legs to go half a second clear of the red cars. Sebastian Vettel will start third ahead of team-mate Kimi Raikkonen. The world champion subsequently escaped punishment for reversing in the pit lane, which could have seen him stripped of pole. But stewards only handed Hamilton a reprimand, after governing body the FIA said \\\"no clear instruction was given on where he should park\\\". Belgian Stoffel Vandoorne out-qualified McLaren team-mate Jenson Button on his Formula 1 debut. Vandoorne was 12th and Button 14th, complaining of a handling imbalance on his final lap but admitting the newcomer \\\"did a good job and I didn't\\\". Mercedes were wary of Ferrari's pace before qualifying after Vettel and Raikkonen finished one-two in final practice, and their concerns appeared to be well founded as the red cars mixed it with the silver through most of qualifying. After the first runs, Rosberg was ahead, with Vettel and Raikkonen splitting him from Hamilton, who made a mistake at the final corner on his first lap. But Hamilton saved his best for last, fastest in every sector of his final attempt, to beat Rosberg by just 0.077secs after the German had out-paced him throughout practice and in the first qualifying session. Vettel rued a mistake at the final corner on his last lap, but the truth is that with the gap at 0.517secs to Hamilton there was nothing he could have done. The gap suggests Mercedes are favourites for the race, even if Ferrari can be expected to push them. Vettel said: \\\"Last year we were very strong in the race and I think we are in good shape for tomorrow. We will try to give them a hard time.\\\" Vandoorne's preparations for his grand prix debut were far from ideal - he only found out he was racing on Thursday when FIA doctors declared Fernando Alonso unfit because of a broken rib sustained in his huge crash at the first race of the season in Australia two weeks ago. The Belgian rookie had to fly overnight from Japan, where he had been testing in the Super Formula car he races there, and arrived in Bahrain only hours before first practice on Friday. He also had a difficult final practice, missing all but the final quarter of the session because of a water leak. Button was quicker in the first qualifying session, but Vandoorne pipped him\"\n",
    "input_text = \"Hi, How are you\"\n",
    "input_ids = tokenizer.encode(input_text[0], return_tensors='pt')\n",
    "\n",
    "\n",
    "generated = batch_beam_search(mambamodel, tokenized_input, 3)\n",
    "tokens = [int(t.item()) for t in generated[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb92a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tokenizer.decode(tokens, skip_special_tokens=True).split(suffix)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917c41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mat] *",
   "language": "python",
   "name": "conda-env-mat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
