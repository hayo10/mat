{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8a750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from _aux_mamba import get_tokenizer, get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba9bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "from operator import itemgetter\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cab64cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaForCausalLM(\n",
      "  (backbone): MambaModel(\n",
      "    (embeddings): Embedding(50280, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x MambaBlock(\n",
      "        (norm): MambaRMSNorm()\n",
      "        (mixer): MambaMixer(\n",
      "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
      "          (act): SiLU()\n",
      "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
      "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
      "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
      "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm_f): MambaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "basemodel = get_model('state-spaces/mamba-130m-hf')\n",
    "print(basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc631cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('state-spaces/mamba-130m-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e3a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"EdinburghNLP/xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e210cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load r2scores\n",
    "file1 = 'experiment/mamba-130m-hf/xsum_r2_scores.pickle'\n",
    "with open(file1, 'rb') as file:\n",
    "    r2_scores = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a86e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the r2_score's key\n",
    "connections = list(r2_scores['score_mat'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c02770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909bb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LayerSelector \n",
    ": select layers included in forwarding.\n",
    "n is the number of selected layers.\n",
    "0th and 24th layers are always included, So we need to select intermediate layers(n-2).\n",
    "\n",
    "I implemented this LayerSelector class that makes all possible selected layer list now.\n",
    "-> this implementation requires too much resources.\n",
    "-> todo : Change this process with an algorithm\n",
    "\n",
    "\"\"\"\n",
    "class LayerSelector:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.selected_layers = []\n",
    "        self.arch = []\n",
    "\n",
    "    def selecting(self, prev, num_selected):\n",
    "        if num_selected == self.n - 2:\n",
    "            self.selected_layers.append(24)\n",
    "            self.selected_layers.insert(0, 0)\n",
    "            self.arch.append(self.selected_layers[:])\n",
    "            self.selected_layers.pop()\n",
    "            self.selected_layers.pop(0)\n",
    "            return\n",
    "\n",
    "        for i in range(prev + 1, 24):\n",
    "            self.selected_layers.append(i)\n",
    "            self.selecting(i, num_selected + 1)\n",
    "            self.selected_layers.pop()\n",
    "\n",
    "    def select_layer(self):\n",
    "        self.selecting(0, 0)\n",
    "        return self.arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9972acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 24], [0, 2, 24], [0, 3, 24], [0, 4, 24], [0, 5, 24], [0, 6, 24], [0, 7, 24], [0, 8, 24], [0, 9, 24], [0, 10, 24], [0, 11, 24], [0, 12, 24], [0, 13, 24], [0, 14, 24], [0, 15, 24], [0, 16, 24], [0, 17, 24], [0, 18, 24], [0, 19, 24], [0, 20, 24], [0, 21, 24], [0, 22, 24], [0, 23, 24]]\n"
     ]
    }
   ],
   "source": [
    "selector = LayerSelector(3)\n",
    "layers = selector.select_layer()\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "481fcf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load_arch\n",
    ": this function takes selected layer numbers and loads real shorcut weights from the shorcut model.\n",
    "\n",
    "linear_layers are list of the real weights.\n",
    "'''\n",
    "def load_arch(selected_layers):\n",
    "    base_path = \"linreg/mamba-130m-hf/xsum\"\n",
    "    linear_layers = []\n",
    "    for i in range(1, len(selected_layers)):\n",
    "        name = f\"{selected_layers[i-1]}_{selected_layers[i]}.pickle\"\n",
    "        full_path = os.path.join(base_path, name)\n",
    "        with open(full_path, 'rb') as file:\n",
    "            linreg = pickle.load(file)\n",
    "            linear_layers.append(linreg)\n",
    "    return linear_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c260459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 24]\n"
     ]
    }
   ],
   "source": [
    "print(layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b8bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n",
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "linear_layers = load_arch(layers[0])\n",
    "for layer in linear_layers:\n",
    "    print(layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d694ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tried to write beam search code\n",
    "I was struggling because huggingface's mamba code doesn't get last state in forward code.\n",
    "If I modify and use customized mamba code then I think we can't use finetuned weights.\n",
    "What should I do?\n",
    "\"\"\"\n",
    "\n",
    "LENGTH_PENALTY = 1.2\n",
    "MIN_LENGTH = 5\n",
    "\n",
    "class SingleBeamSearchSpace():\n",
    "\n",
    "    def __init__(self, decoder_input, beam_size, max_length = 255):\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        super(SingleBeamSearchSpace, self).__init__()\n",
    "\n",
    "        self.device = decoder_input.device\n",
    "        self.word_indice = [torch.LongTensor(self.beam_size).zero_().to(self.device)]\n",
    "        self.prev_beam_indice = [torch.LongTensor(self.beam_size).zero_().to(self.device) - 1]\n",
    "        self.cumulative_probs = [torch.FloatTensor([.0] + [-float('inf')] * (beam_size - 1)).to(self.device)]\n",
    "        self.masks = [torch.ByteTensor(beam_size).zero_().to(self.device)] # 1 if it is done else 0\n",
    "        self.decoder_input = decoder_input\n",
    "\n",
    "        self.decoder_input_ids = self.decoder_input.repeat(self.beam_size, 1)\n",
    "        #self.decoder_attention_masks = self.decoder_input['attention_mask'].repeat(self.beam_size, 1)\n",
    "\n",
    "        self.current_time_step = 0\n",
    "        self.done_cnt = 0\n",
    "\n",
    "    def get_length_penalty(self, length, alpha = LENGTH_PENALTY, min_length = MIN_LENGTH):\n",
    "        p = (1 + length) ** alpha / (1 + min_length) ** alpha\n",
    "\n",
    "        return p\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.done_cnt >= self.beam_size:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def get_batch(self):\n",
    "        y_prev = self.word_indice[-1].unsqueeze(-1)\n",
    "        \n",
    "        return y_prev\n",
    "\n",
    "    def collect_result(self, y_hat):\n",
    "        output_size = y_hat.size(-1)\n",
    "\n",
    "        self.current_time_step += 1\n",
    "\n",
    "        cumulative_prob = y_hat + self.cumulative_probs[-1].masked_fill_(self.masks[-1], -float('inf')).view(-1, 1, 1).expand(self.beam_size, 1, output_size)\n",
    "        top_log_prob, top_indice = torch.topk(cumulative_prob.view(-1), self.beam_size, dim = -1)\n",
    "        # |top_log_prob| = (beam_size)\n",
    "        # |top_indice| = (beam_size)\n",
    "        self.word_indice += [top_indice.fmod(output_size)]\n",
    "        self.prev_beam_indice += [top_indice.div(output_size).long()]\n",
    "\n",
    "        self.cumulative_probs += [top_log_prob]\n",
    "        self.masks += [torch.eq(self.word_indice[-1], tokenizer.eos_token_id)]\n",
    "        self.done_cnt += self.masks[-1].float().sum()\n",
    "\n",
    "        #self.prev_state = torch.index_select(prev_state, dim = 1, index = self.prev_beam_indice[-1]).contiguous()\n",
    "\n",
    "    def get_n_best(self, n = 1):\n",
    "        sentences = []\n",
    "        probs = []\n",
    "        founds = []\n",
    "\n",
    "        for t in range(len(self.word_indice)):\n",
    "            for b in range(self.beam_size):\n",
    "                if self.masks[t][b] == 1:\n",
    "                    probs += [self.cumulative_probs[t][b] / self.get_length_penalty(t)]\n",
    "                    founds += [(t, b)]\n",
    "\n",
    "        for b in range(self.beam_size):\n",
    "            if self.cumulative_probs[-1][b] != -float('inf'):\n",
    "                if not (len(self.cumulative_probs) - 1, b) in founds:\n",
    "                    probs += [self.cumulative_probs[-1][b]]\n",
    "                    founds += [(t, b)]\n",
    "\n",
    "        sorted_founds_with_probs = sorted(zip(founds, probs), \n",
    "                                            key = itemgetter(1), \n",
    "                                            reverse = True\n",
    "                                            )[:n]\n",
    "        probs = []\n",
    "\n",
    "        for (end_index, b), prob in sorted_founds_with_probs:\n",
    "            sentence = []\n",
    "\n",
    "            for t in range(end_index, 0, -1):\n",
    "                sentence = [self.word_indice[t][b]] + sentence\n",
    "                b = self.prev_beam_indice[t][b]\n",
    "\n",
    "            sentences += [sentence]\n",
    "            probs += [prob]\n",
    "\n",
    "        return sentences, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee97dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_beam_search(model, tokenized_input, beam_size, max_length = 255, n_best = 1):\n",
    "    model.eval()\n",
    "    x = tokenized_input\n",
    "    batch_size = x.size(0)\n",
    "\n",
    "    # initialize beam-search.\n",
    "    spaces = [SingleBeamSearchSpace(tokenized_input, \n",
    "                                        beam_size=beam_size\n",
    "                                        ) for i in range(batch_size)]\n",
    "    done_cnt = [space.is_done() for space in spaces]\n",
    "\n",
    "    length = 0\n",
    "    while sum(done_cnt) < batch_size and length <= max_length:\n",
    "        # current_batch_size = sum(done_cnt) * beam_size\n",
    "\n",
    "        # initialize fabricated variables.\n",
    "        fab_input = []\n",
    "        fab_h_src, fab_mask = [], []\n",
    "\n",
    "        # batchify.\n",
    "        for i, space in enumerate(spaces):\n",
    "            if space.is_done() == 0:\n",
    "                y_prev_ = space.get_batch()\n",
    "\n",
    "                fab_input += [y_prev_]\n",
    "\n",
    "                fab_h_src += [x[i, :]] * beam_size\n",
    "                #fab_mask += [input_mask[i, :]] * beam_size\n",
    "\n",
    "        fab_input = torch.cat(fab_input, dim = 0)\n",
    "        fab_h_src = torch.stack(fab_h_src)\n",
    "\n",
    "        fab_output_logits = model(fab_input).logits[:, -1, :]\n",
    "        \n",
    "        y_hat = torch.log_softmax(fab_output_logits, dim=-1)\n",
    "        output_size = y_hat.shape[-1]\n",
    "\n",
    "        cnt = 0\n",
    "        for space in spaces:\n",
    "            if space.is_done() == 0:\n",
    "                from_index = cnt * output_size\n",
    "                to_index = (cnt + 1) * output_size\n",
    "\n",
    "                # pick k-best results for each sample.\n",
    "                space.collect_result(y_hat[from_index:to_index])\n",
    "                cnt += 1\n",
    "\n",
    "        done_cnt = [space.is_done() for space in spaces]\n",
    "        length += 1\n",
    "\n",
    "    batch_sentences = []\n",
    "    batch_probs = []\n",
    "\n",
    "    for i, space in enumerate(spaces):\n",
    "        sentences, probs = space.get_n_best(n_best)\n",
    "\n",
    "        batch_sentences += [sentences]\n",
    "        batch_probs += [probs]\n",
    "\n",
    "    return batch_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3d1ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortcutModel(nn.Module):\n",
    "    def __init__(self, n, selected_layers):\n",
    "        super(ShortcutModel, self).__init__()\n",
    "        self.n = n\n",
    "        self.path = nn.ModuleList([nn.Linear(768, 768) for _ in range(self.n-1)])\n",
    "        self.selected_layers = selected_layers\n",
    "        self.weight_list = []\n",
    "        self.tokenizer = get_tokenizer('state-spaces/mamba-130m-hf')\n",
    "        self.basemodel = get_model('state-spaces/mamba-130m-hf')\n",
    "        self.embed = basemodel.backbone.embeddings\n",
    "        self.norm = basemodel.backbone.norm_f\n",
    "        self.lm_head = basemodel.lm_head\n",
    "        \n",
    "        \n",
    "    def load_weights(self):\n",
    "        base_path = \"linreg/mamba-130m-hf/xsum\"\n",
    "        for i in range(1, self.n):\n",
    "            name = f\"{self.selected_layers[i-1]}_{self.selected_layers[i]}.pickle\"\n",
    "            full_path = os.path.join(base_path, name)\n",
    "            with open(full_path, 'rb') as file:\n",
    "                linreg = pickle.load(file)\n",
    "                self.weight_list.append(linreg)\n",
    "    \n",
    "    \n",
    "    def initialize(self):\n",
    "        for i in range(self.n-1):\n",
    "            self.path[i].weight.data = self.weight_list[i]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        for layer in self.path:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.lm_head(x)\n",
    " \n",
    "        return x\n",
    "\n",
    "    def original(self, x):\n",
    "        x = self.basemodel(x)\n",
    "        return x\n",
    "    \n",
    "#     def decode(self, x):\n",
    "#         result_list = self.result.tolist()\n",
    "#         self.decoded_result = self.tokenizer.decode(result_list, skip_special_tokens=True).split(suffix)[1]\n",
    "#         self.decoded_ref = self.tokenizer.decode(x, skip_special_tokens=True).split(suffix)[1]\n",
    "#         print(self.decoded_result)\n",
    "#         print(self.decoded_ref)\n",
    "        \n",
    "    def get_score(self, x):\n",
    "        rouge = evaluate.load('rouge')\n",
    "        scores = rouge.compute(predictions=self.result, references=x)\n",
    "        print(scores['rouge2'])\n",
    "        return scores['rouge2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97d62d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdfd878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = ShortcutModel(3, layers[0])\n",
    "# model.load_weights()\n",
    "# model.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03357887",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = [\n",
    "    \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n",
    "]\n",
    "prefix = \"summarize this: \"\n",
    "suffix = \"Here's the summary: \"\n",
    "input_text = prefix + dataset['train']['document'][0]+suffix\n",
    "max_length = 512\n",
    "tokenized_input = tokenizer.encode(input_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072347ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129433/124664633.py:52: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1772.)\n",
      "  cumulative_prob = y_hat + self.cumulative_probs[-1].masked_fill_(self.masks[-1], -float('inf')).view(-1, 1, 1).expand(self.beam_size, 1, output_size)\n"
     ]
    }
   ],
   "source": [
    "generated = batch_beam_search(model, tokenized_input, 3)\n",
    "tokens = [int(t.item()) for t in generated[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb92a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tokenizer.decode(tokens, skip_special_tokens=True).split(suffix)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917c41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
